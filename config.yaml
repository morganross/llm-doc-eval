# llm-doc-eval configuration (bootstrap defaults; used when FPF wiring is enabled)
fpf:
  path: ""  # optional; if empty, resolver will default to ../FilePromptForge

llm_api:
  max_concurrent_llm_calls: 4
  timeout_seconds: 120

retries:
  attempts: 3
  base_delay_seconds: 2
  max_delay_seconds: 10
  jitter: true



models:
  model_a:
    provider: openai
    model: gpt-4.1-mini
  model_b:
    provider: google
    model: gemini-2.5-flash

single_doc_eval:
  trial_count: 1
  criteria_file: criteria.yaml

pairwise_eval:
  trial_count: 1
  criteria_file: criteria.yaml


# Evaluation mode selection (optional)
# - single: run only single-document graded evaluation
# - pairwise: run only pairwise comparisons
# - both: run single first, then pairwise
evaluation:
  mode: both
