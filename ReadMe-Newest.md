# LLM Document Evaluation (llm-doc-eval) Project Overview

The `llm-doc-eval` project provides a robust framework for evaluating and comparing documents generated by Large Language Models (LLMs) based on predefined criteria. Built on Langchain, it facilitates structured evaluation outputs, supports both single-document grading and pairwise comparisons, and includes an Elo ranking system to identify top-performing documents. All evaluation results are stored persistently in SQLite databases and can be exported to CSV format for further analysis.

## Key Components

*   **`llm_doc_eval/cli.py`**: The primary command-line interface (CLI) for `llm-doc-eval`. It exposes various subcommands to perform different types of evaluations, summarize results, and export data. This is the main entry point for users interacting with the evaluation system.
*   **`llm_doc_eval/api.py`**: Provides the programmatic interface to the core evaluation functionalities. It exposes functions like `run_pairwise_evaluation`, `run_single_evaluation`, `run_evaluation`, and `get_best_report_by_elo` that can be imported and used by other Python applications (e.g., as seen in the `acm` project's `evaluate.py`).
*   **`config.yaml`**: The main configuration file for `llm-doc-eval`. It defines:
    *   `fpf`: Configuration related to FilePromptForge integration (e.g., `path`).
    *   `llm_api`: Settings for LLM API calls, including `max_concurrent_llm_calls`, `timeout_seconds`, and `retries` parameters.
    *   `models`: A listing of LLM models with their `provider` and `model` IDs, used for the evaluation process itself.
    *   `single_doc_eval`: Configuration for single-document evaluations, including `trial_count` and the `criteria_file` to use.
    *   `pairwise_eval`: Configuration for pairwise evaluations, including `trial_count` and the `criteria_file` to use.
    *   `evaluation`: Defines the default `mode` (e.g., `single`, `pairwise`, `both`) for evaluations.
*   **`criteria.yaml`**: This file defines the specific criteria used for evaluating documents. It lists aspects such as "factuality," "relevance," "completeness," and "style_clarity," which LLMs are instructed to assess during the evaluation process. These criteria can be extended with `name`/`description`/`weight` objects for more granular control.
*   **SQLite Database**: Evaluation results are automatically stored in an SQLite database (default: `DB_PATH`). Each evaluation run creates a unique database file named `results_YYYYMMDD_HHMMSS.sqlite` for historical tracking.

## Core Functionality Highlights

*   **Single-Document Grading**: Evaluate individual documents against a set of criteria.
*   **Pairwise Comparison**: Compare two documents directly to determine which is superior based on the defined criteria.
*   **Combined Evaluation**: Run both single-document grading and pairwise comparisons in a single execution.
*   **Elo Ranking**: Computes an Elo rating for documents based on pairwise comparison results, providing a ranked list of best-performing candidates.
*   **Flexible Configuration**: LLM models, API call parameters, and evaluation criteria are highly configurable via YAML files.
*   **Data Export**: All evaluation results (single-doc, pairwise, and Elo summaries) can be exported to CSV files for easy integration with other analysis tools.

## Getting Started and Usage (CLI)

The `llm-doc-eval` CLI provides several subcommands to manage evaluation tasks.

**General Usage Pattern:**

```bash
python -m llm_doc_eval.cli <command> [options]
```
(Note: If running from the `llm-doc-eval` directory, you might directly use `python cli.py <command> [options]`.)

### Subcommands

#### 1. `run-pairwise`

Runs pairwise evaluation over a folder of candidate documents. The LLM compares documents in pairs based on the configured criteria.

*   **Arguments:**
    *   `--docs <path>` (Required): Path to the folder containing candidate report files (e.g., markdown, text files).
    *   `--db <path>`: Optional path to the SQLite database. Defaults to `(llm_doc_eval_package_root)/llm_doc_eval/llm_doc_eval.sqlite`.
    *   `--config <path>`: Optional path to a custom `config.yaml`.
    *   `--criteria <path>`: Optional path to a custom `criteria.yaml`.
    *   `--export-dir <path>`: Directory to write CSV exports. Defaults to `exports/` next to the DB.
    *   `--no-export`: Flag to suppress CSV exports after the run.
*   **Example:**
    ```bash
    python -m llm_doc_eval.cli run-pairwise --docs /path/to/generated_reports --db my_evals.sqlite
    ```

#### 2. `run-single`

Runs single-document grading over a folder of candidate documents. Each document is evaluated individually against the criteria.

*   **Arguments:** (Same as `run-pairwise`)
*   **Example:**
    ```bash
    python -m llm_doc_eval.cli run-single --docs /path/to/generated_reports --criteria custom_criteria.yaml
    ```

#### 3. `run-both`

Runs both single-document grading and pairwise evaluation in sequence. This combines the functionalities of `run-single` and `run-pairwise`.

*   **Arguments:** (Same as `run-pairwise`)
*   **Example:**
    ```bash
    python -m llm_doc_eval.cli run-both --docs /path/to/generated_reports --export-dir ./my_exports --no-export
    ```

#### 4. `summary`

Computes and displays the Elo ranking from existing pairwise evaluation results in a SQLite database.

*   **Arguments:**
    *   `--db <path>`: Optional path to the SQLite database.
    *   `--out <path>`: Optional CSV output path for the summary. If not provided, prints to console.
*   **Example (print to console):**
    ```bash
    python -m llm_doc_eval.cli summary --db my_evals.sqlite
    ```
*   **Example (export to CSV):**
    ```bash
    python -m llm_doc_eval.cli summary --db my_evals.sqlite --out elo_ranking.csv
    ```

#### 5. `export`

Exports a specific table from an SQLite database to a CSV file.

*   **Arguments:**
    *   `--db <path>` (Required): Path to the SQLite database.
    *   `--table <name>` (Required): Table name (e.g., `pairwise_results` or `single_doc_results`).
    *   `--out <path>` (Required): CSV output path.
*   **Example:**
    ```bash
    python -m llm_doc_eval.cli export --db my_evals.sqlite --table pairwise_results --out pairwise_results.csv
    ```

## Audience-Specific Information

### For Developers

*   **Programmatic Access**: The `llm_doc_eval/api.py` module is designed for direct integration into other Python applications. Use `run_single_evaluation`, `run_pairwise_evaluation`, or `run_evaluation` to programmatically trigger evaluations.
*   **Extending Evaluation Logic**: To introduce new evaluation methods or customize existing ones, delve into the `llm_doc_eval/engine` directory, which likely contains the core logic for how LLMs perform grading.
*   **Model Integration**: Add new LLM providers or specific models by extending the `models` section in `config.yaml`. Ensure that the corresponding Langchain integrations are available and configured correctly.
*   **Custom Criteria**: `criteria.yaml` allows for detailed definition of evaluation criteria. More complex criteria can include `name`, `description`, and `weight` fields.

### For Users / Researchers

*   **Organizing Documents**: Place all documents you wish to evaluate within a single directory, which you'll pass to the `--docs` argument. The system will automatically discover and process markdown or text files within that folder.
*   **Customizing Criteria**: Define a `criteria.yaml` file to precisely control what aspects of the documents are evaluated. This is critical for evaluations tailored to specific research questions.
*   **Analyzing Results**: Use the `summary` command to get an immediate Elo ranking. For detailed analysis, `export` the `single_doc_results` and `pairwise_results` tables to CSV and load them into spreadsheets or data analysis tools.
*   **Identifying Best Reports**: The `get_best_report_by_elo` function (accessible via CLI `run-pairwise` and `run-both` outputs, or directly through `api.py`) provides an objective way to select the top-performing document.

### For Operators / Administrators

*   **Resource Management**: Monitor LLM API usage and costs. Adjust `max_concurrent_llm_calls` and `timeout_seconds` in `config.yaml` to manage concurrency and prevent API abuse or timeouts.
*   **Retry Mechanisms**: The `retries` section in `config.yaml` configures how API calls are retried in case of transient errors, improving system robustness.
*   **Data Archiving**: Regularly back up the SQLite databases generated by evaluations. These contain valuable historical data about document performance.
*   **Configuration Consistency**: Ensure that all instances of `llm-doc-eval` use consistent `config.yaml` and `criteria.yaml` files, especially in a production pipeline. The `--config` and `--criteria` CLI arguments allow overriding defaults for specific runs.
